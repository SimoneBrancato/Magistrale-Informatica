# Big Data

[TOC]

## 1  Introduzione

 I dati al giorno d'oggi vengono prodotti velocemente, sono eterogenei e ricchi di informazioni. Esiste la regola delle 3 V:

- Volume: vanno da pochi MB a PB
- Velocità: da batch, a periodici fino a real time
- Varietà: tabelle, database, log, foto, audio, social e non strutturati

In generale si definisce come big data qualsiasi dato la cui dimensione è un problema per operazioni di conservazione, trasmissione ed elaborazione. Supponiamo di voler analizzare 100 TB di dati, ovviamente non conviene farlo su una sola macchina. Pertanto l'idea è quella di scalare orizzontalmente. Questo comporta la richiesta di più spazio fisico, costi energetici e rete di comunicazione, ma migliora i costi, fault tolerance e scalabilità.

Distinguiamo la High Performance Computing e la Big Data Computing. La HPC funziona come un cluster di macchine indipendenti dette nodi, che comunicano in una rete ad alta velocità con accesso ad uno storage centralizzato. Dato che ogni macchina ha una sua RAM, si dice architettura a memoria distribuita. Avendo diversi nodi questi devono coordinarsi correttamente, ciò viene fatto dal nodo master che si occupa dell'orchestrazione, mentre i nodi worker si occupano del calcolo effettivo. Possono comunicare con Message Passing Interface. Il problema è che in questo modo dato che la memoria di massa è centralizzata questa non scala facilmente, pertanto nasce l'architettura dei sistemi big data, dove ogni nodo ha il suo storage e l'unica cosa che li lega è la rete di comunicazione. Ovviamente, devono agire come cluster, pertanto dobbiamo adottare dei File System distribuiti come Hadoop Distribuited File System, e sistemi di calcolo distribuito come Spark.

Link utile: https://colab.research.google.com/

Esame: Scritto 25%, Progetto 75% da consegnare entro 60 giorni dal superamento dello scritto, laboratorio in aula che dà 3 punti extra.



## 2  Map Reduce 

Serve come piattaforma di storage ed analisi di big data, permette a programmatori senza esperienza di sistemi distribuiti di utilizzare le risorse di un data center per elaborare grandi moli di dati. 

Il sistema Hadoop è composto da:

- MapReduce Runtime (coordinatore): paradigma per programmazione distribuita
- Hadoop Distribuited FIle System (HDFS): per permettere alle macchine di agire come cluster, implementando ridondanza dei dati e fault tolerance

Ovviamente nasce dall'esigenza di dover scalare orizzontalmente piuttosto che verticalmente. Vogliamo processare molti dati, in maniera distribuita e semplice. Questa soluzione è proprio quella di MapReduce. Si utilizza un file system distribuito, dividendo i file in chunk, ognuno di essi verrà distribuito sul cluster e replicato. Si ragionerà in termini di nodi master, in HDFS è Name Node che appunto memorizza metadati di dove si trovano i chunks dei file, e i Data Node, che immagazinano e reuperano i blocchi se richiesto.

Le sfide sono molte, come per esempio

- Map
  - Come itero su molti dati in parallelo?
  - Come estraggo informazioni per ogni iterazione?
  -  Come ordino i risultati?
- Reduce
  - Come aggrego i dati intermedi?
  - Come genero l'output finale?

Il paradigma MapReduce fonda le radici nella programmazione funzionale, dove Map prende una funzione e la applica ad ogni elemento, mentre Fold applica iterativamente una funzione  per aggregare i risultati. In MapReduce si ha:

- Map: prende in input un oggetto key-value $(k,v)$ e restituisce un elenco di coppie $(k_1,v_1),...,(k_n,v_n)$
- Group by key: colleziona le coppie con stessa chiave $k$ e vi associa tutti i valori.
- Reduce: prende in input l'elemento $(k, [v_1,...,v_n])$ e li combina in un certo modo.

La pipeline è sempre la stessa, variano le funzioni applicate da Map e Reduce.

Più nello specifico, la fase Map prende $n$ elementi key-value, e ne restituisce $m$ intermedi. Questi vengono raggruppati per chiave, creando elementi con chiave e una lista di valori associato, e la Reduce riduce la lista rimanente restituendo elementi key-value.

Un tipico problema risolvibile con MapReduce è il word counting, altamente parallelizzabile.

<img src=".\img\wordcount.png" style="zoom:67%;" />

Vediamo il codice:

```python
map(key, value):
    # key: document name, value: text
    for w in value:
        emit(w,1)

reduce(key, values):
	# key: a word, value: an iterator over counts
    result = 0
    for v in values:
        result += v
    emit(key, result)
```

L'ambiente MapReduce si occupa di:

- Partizionare i dati in input
- Schedula l'esecuzione del codice sulle varie macchine
- Effettua la group by key
- Gestisce i crash delle macchine
- Gestisce la comunicazione tra macchine

Un altro esempio di problema risolvibile con MapReduce è contare quante parole di una certa lunghezza esistono in una collezione di documenti, l'input è proprio l'insieme dei documenti.

```python
 map(key, value):
     # key: document name; value: text of the document
     for w in value:
     	emit(length(w), 1)

 reduce(key, values):
 	# key: a word; value: an iterator over counts
    result = 0
    for w in values:
        result += 1
    emit(key, result)
```

Ovviamente input e output finale verranno memorizzate nel file system distribuito, mentre i risultati intermedi vengono memorizzati sul file system locale dei worker.

Il nodo che coordina i worker è il Master. Ogni task può essere:

- Idle: pronto ad essere eseguito ma non assegnato
- In-progress: in esecuzione su un worker
- Completato: terminato con successo e risultati pronti

Quando un worker diventa disponibile il Master gli assegna immediatamente un **task idle** da eseguire.

Dopo che un **Map Task** ha completato la sua esecuzione, deve generare dei file intermedi che saranno utilizzati dai **Reduce Task**.

- Ogni **Map Task** scrive più file intermedi, chiamati **R file**, uno per ogni Reduce Task.

- Quando il Map Task termina, comunica al Master:

  - **La posizione** dei file intermedi (es. su HDFS o su un nodo specifico).
  - **La dimensione** dei file intermedi.

  Così il Master può pusharli ai reducer.

Periodicamente il Master invia ping periodici ai Worker per verificare che siano attivi, se uno di essi non risponde allora è crashato e il Master può riassegnare i task ad altri Worker attivi.

Si sceglie il numero di Map task M molto maggiore del numero di nodi del cluster così che ogni nodo possa eseguire più Map Task in parallelo e migliora il load balancing. Il numero di Map Task viene determinato dal numero di chunk Distribuited File System (DFS) quindi per esempio se ci sono per un file 100 chunks allora 100 Map Tasks. 

In genere il numero di Reduce Task R è più piccolo di M perchè l'output del Reduce verrà scritto su R file e se ce ne sono troppi è inefficace, inoltre i Reduce Task raccolgono più dati per produrre un risultato più aggregato.

slide 38





skip da hadoop fino  a matrix-vector multiplication



## 3  Spark

test




# Machine Learning

[TOC]

## 1  Introduzione

Il machine learning consiste in quel campo di studi che permette ai computer di imparare senza essere esplicitamente programmati. I relativi algoritmi sono in grado di apprendere come risolvere uno specifico problema a partire da un set di dati di esempio.

Un computer apprende dall'esperienza rispetto un task se la misura delle performance migliorano con l'esperienza rispetto al task svolto.

Il task è il problema da risolvere. L'esperienza sono i dati, ognuno di essi è chiamato esempio. Le performance sono una misura che riesca a valutare se il modello è buono rispetto i dati in input, si definisce come $P(\{y^{(i)}\}^m_{i=1}, \{\hat y^{(i)}\}^m_{i=1})$, questo può essere alto o basso. Comunque sia $P\in[0,1]$, di conseguenza definiamo la misura dell'errore $1-P$. 

Per risolvere problemi di machine learning definiamo modelli statistici che dipendono dal task. Un esempio $x=(x_1, x_2, ..., x_d)$, e se ho tanti esempi definisco l'$i$-esimo esempio  $x^{(i)}=(x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_d)$. Le componenti $x_n$ vengono dette *features*. 

Osservato un problema è bene capire le caratteristiche importanti rispetto al task da risolvere, pertanto non esiste una rappresentazione universale. L'estrazione delle features mette in luce caratteristiche salienti dei dati trascurandone altri. Immaginiamo di avere un grafico con assi numero di parole chiave spam ed errori ortografici, possiamo aspettarci che email di spam abbiano tanti errori e parole chiave spam, diversamente quelle non spam.

Gli attributi possono essere:

- **Categorical:** nominale (non c'è un ordine relativamente alle features, come il colore) o ordinale (quindi con ordine più o meno espresso, come low, middle e high).
- **Continuous:** numerico

I tipi di task sono:

- **Classificazione:** prendere un input $x\in R^d$ e stabilire a quale categoria appartiene quindi $y \in \{1,...,k\}$ dove $k$ è il numero di classi. Vogliamo definire un classificatore $h_v(x)=y$ con $h:R^4 \rightarrow \{1,...,k\}$. Un altro esempio è determinare quante macchine sono presenti in un immagine, le classi sono i possibili output.
- **Regressione**

Diversi sono anche i tipi di learning:

- **Supervised Learning:** vengono dati in pasto all'algoritmo sia l'input che l'output associato, così vengono risolti classificazione e regressione. 
- **Unsupervised Learning:** qui viene dato all'algoritmo di learning solo l'input senza quindi nessuna etichetta. Il loro obiettivo è stimare come i dati si distribuiscono quindi sulla probabilità di riscontrare un certo input. 
- **Reinforcement Learning:** non verrà trattato in questo corso, si attua una tecnica di premio e penalità rispetto al raggiungimento di un determinato obiettivo.

**Parametri e Iperparametri:** Durante la fase di training si modificano i parametri per alterare il comportamento del modello. Invece, ci sono valori che non possono essere aggiustati dalla fase di training e sono chiamati iperparametri, serve ottimizzarli prima della fase di training, spesso per tentativi.

In genere abbiamo un dataset che viene rappresentato con una design matrix
$$
x^{(1)}= x^{(1)}_1\ ... \ x^{(1)}_d \\
x^{(2)}= x^{(2)}_1\ ... \ x^{(2)}_d \\
... \\
x^{(m)}= x^{(m)}_1\ ... \ x^{(m)}_d \\
$$

$$
\begin{bmatrix}
x^{(1)}_1 & x^{(1)}_2 & \cdots & x^{(1)}_d \\
x^{(2)}_1 & x^{(2)}_2 & \cdots & x^{(2)}_d \\
\vdots & \vdots & \ddots & \vdots \\
x^{(m)}_1 & x^{(m)}_2 & \cdots & x^{(m)}_d \\
\end{bmatrix}
$$
Le colonne sono le features, ogni riga è un dato e tale matrice è $X \in R^{m\cdot d}$. Un'altra matrice è $Y\in A^{m\cdot k}$ dove $A=\{1,...,k\}$ In genere $k=1$, pertanto $Y$ rappresenta le classi di appartenenza di ogni dato.

Passando alla procedura di learning, spesso detta di training, si ha in input 

- Dataset di training 
- Modello con i relativi parametri
- Misura delle performance

Per sapere se l'algoritmo funziona, alla ricezione di nuovi dati l'algoritmo deve saper generalizzare su un nuovo set di dati, detto test set. Qualora avessimo anche gli iperparametri, usiamo anche il dataset di validation. Con i dati di training otteniamo i parametri del modello, con il dataset di validation otteniamo gli iperparametri (indipendenti dal modello) e infine il dataset di test per ottenere l'accuracy.

Un algoritmo di classificazione per classificare $x_1$ potrebbe essere:

```pseudocode
if x1 > theta then "spam"
else "not spam"
```

Sulla base di theta avrò diversa accuracy. Quindi l'algoritmo di learning deve determinarlo. Un metodo naive potrebbe essere provare tutti i theta e verificare quale accuracy è migliore, purtroppo è un'euristica non intelligente perchè l'algoritmo di learning non impara più dai dati, ma dai possibili parametri. Questo processo va bene su una o due dimensioni, ma ovviamente non scala e diventa troppo complesso.

Cerchiamo adesso di normalizzare i dati, per esempio $[0,1]$. Tale normalizzazione la facciamo sui dati di training, e l'algoritmo verrà allenato sui nuovi dati normalizzati.

La pratica della normalizzazione nel ML è fondamentale, consiste nello scalare il dato così da analizzarlo all'interno di uno specifico range, come per esempio $[0,1]$. Possiamo adoperare una normalizzazione Min-Max, cioè

$$
\hat x_j = \frac{x_j-x_j^{min}}{x_j^{max}-x_j^{min}}\in [0,1]
$$
Ovviamente, il dataset di test dovrà pure essere trasformato, per poterlo confrontare col dominio di appartenenza dell'algoritmo.

In realtà il più utilizzato è lo Z-score, dove si utilizza la formula:
$$
Z = \frac{X-\mu_j}{\sigma_j}
$$
In questo modo i dati avranno media zero e deviazione standard 1. Definiamo infatti:

- **Media:** $\mu_j=\frac1m\sum_{i=1}^mx_j^{(i)}$
- **Deviazione Standard:** $\sigma_j=\sqrt{\frac1m\sum_{i=1}^{m}x_j^{(i)}-\mu_j}$



**Il paradigma Training/Test**

Una produra di training tenta di ottimizzare le performance dell'algoritmo sui dati disponibili, ma non è garantito che dopo la fase di training performerà bene su dati che non ha mai visto.

- Training: usiamo la procedura di learning per ottimizzare i parametri dell'algoritmo di machine learning
- Testing: usiamo l'algoritmo allenato per processare i dati di test e valutare le performance dell'algoritmo.

Come detto gli algoritmi di machine learning hanno anche degli iperparametri che non sono ottimizzati in fase di training. Per sistemarli, possiamo dividere il dataset in 3 insiemi, training, validation e test. Quello di validazione serve a selezionare gli iperparametri migliori tale fase viene chiamata fase di tuning.

![](.\img\tuning.png)

Ora, mettiamo in ordine tutto. Abbiamo un dataset, lo dividiamo in training data e test data. Successivamente dividiamo ulteriormente il training data in chunks, scegliamo uno dei chunk come validation fold, il resto verrà usato come training set vero e proprio. Tale processo può essere ripetuto k volte dove k è il numero di chunk. Una volta trovato il miglior setting di iperparametri, il modello può essere allenato nuovamente sull'intero training set e testato sul dataset.



### 1.1 Componenti del ML

Quello che vogliamo fare è determinare un modello che funga da confine di decisione, dividendo i dati in classi. I dati saranno $x \in R^d$, e le etichette $y=\{0,1\}$. Da qui vogliamo sapere la probabilità di assegnare un etichetta a un dato, cioè $P(y|x)$ quindi una probabilità a posteriori.

Gli ingredienti del machine learning sono:

1. Modello
2. Loss Function
3. Algoritmo di learning
4. Performances evaluation

Dobbiamo capire come possiamo noi umani imparare, e il componente principale del cervello è il neurone, la più semplice macchina compotazionale.

### 1.2  Modello di McColloch-Pitt

L'idea è quella di emulare il comportamento di un neurone, quindi:

1. Un'unità computazionale che elabora $n$ input binari
2. Il modello restituisce un output binario
3. Ogni neurone è associato ad un parametro $ \theta$ ovvero threshold (soglia)
4. L'output può essere connesso ad altri neuroni
5. L'input può provenire da altri neuroni o essere un vettore binario o una computazione
6. Due tipi di input: eccitatorio o inibitorio

<img src=".\img\neur.png" style="zoom:80%;" />

- Se almeno un input è $m\ge 1$ e $\exists y_i|y_i=1$ allora output 0. quindi se almeno uno degli inibitori è 1 il neurone è non attivo
- Se $\sum_{i=1}^dx_i\ge \theta$ allora output 1
- Altrimenti output 0

Se abbiamo un neurone con due input eccitatori e soglia 2 allora il modello è la sommatoria di due input quindi $x_1+x_2\ge \theta$ e dato che la soglia è 2 allora si comporta come la funzione AND. 

La threshold ce la da la funzione oracolo, quindi dobbiamo conoscere l'output voluto. Sembra difficile ma in effetti nei problemi di supervised learning è possibile.

![](.\img\neuro.png)

Possiamo fare anche delle cose più interessanti, come per esempio se abbiamo un neurone con un input inibitorio e un input eccitatorio con threshold 1 allora stiamo computando $x_1+\overline x_2 \ge 1$. Tale modello, come notiamo, è parametrizzato, dove i parametri sono gli input e le threshold.

Ovviamente questo modello ha dei limiti, infatti:

1.  è pensato per ricevere input binario e non numeri interi
2. è pensato per funzioni logiche.
3. I parametri sono fissati a mano
4. Non esistono pesi, due input hanno stessa importanza
5. Tutte le funzioni sono linearmente separabili, cioè le classi sono descrivibili sono solo modelli lineari

![](.\img\andornot.png)

### 1.3  Modello del percettrone - Rosenblatt

Consiste nella naturale evoluzione del modello del neurone. Il percettrone può essere visto come un'unità di elaborazione dati, che riceve diversi input, non abbiamo input inibitori ma abbiamo un peso associato ad ogni input detto peso. Tale modello è in grado di computare la combinazione lineare degli input. L'output va in una nuova unità detta step function, dove abbiamo un altro parametro che definirà la threshold dando un output binario

![](.\img\percep.png)

Tale modello computa:
$$
f(x_1, ..., x_d)= \begin{cases} 1\ \ \ \ se\ \sum_{j=1}^dw_jx_J>\theta_0 \\
\\
0\ \ \ \  \text{altrimenti}
\end{cases}
$$
Sostanzialmente gli input appartengono all'insieme dei reali.

Possiamo modificare tale modello spostando la step function all'input si ha che la sommatoria deve essere maggiore di 0 per essere pari a 1. Se avessimo 3 dimensioni il confine di separazione non sarebbe una linea ma un piano. 

Stiamo computando $\sum x_j\theta_j>0$, con il possibile output che è una classificazione binaria. Se $y=0$ e $\hat y = 1$ devo decrementare i parametri per permettere di allinearci alla stima giusta. Può succedere il contrario, quindi voglio in output 1 ma la stima è zero, significa che la sommatoria non supera lo zero e dobbiamo incrementare i pesi delle feature.
$$
\theta_j \leftarrow \theta_{j}+(y-\hat y)x_j
$$
Questa formula funziona in entrambi i casi sopracitati sfruttando gli errori che abbiamo. Quello che fa è modificare il peso in base all'errore. L'operazione che faremo è $\theta \leftarrow \theta_j  \pm \epsilon$.

I passi sono:

1. Inizializza i pesi random in [0,1]
2. Considera $x^{(i)}$ e computa $\hat y^{(i)}$ e questo $\forall i$
3. Per le predizioni errate aggiusta i pesi:
   - Se $y=0$ e $\hat y = 1$ decrementa
   - Se $y=1$ e $\hat y = 0$ incrementa
4. Ritorna al passo 2 fino a convergenza



### 1.4  XOR Problem

Questa categoria di problemi non è rappresentabile con modelli lineari a causa del loro grafico

![](.\img\xor.png)

Pensiamo di avere una rete neurale a percettroni così fatta:

![](.\img\rete.png)
$$
f_1(x_1,x_2)= [x_1-x_2>0.5] \\
f_2(x_1,x_2)= [x_2-x_1>0.5] \\ 
f_3(f_1(x_1,x_2), f_2(x_1,x_2)) = 1*f_1(x_1,x_2) + 1*f_2(x_1,x_2) > 0.5
$$
Tale modello descrive proprio la soluzione al problema dello XOR. Infatti le tabelle di verità corrispondono. L'input praticamente viene trasformato e dato in input al terzo neurone descritto da $f_3$. 

![](.\img\xor2.png)

Dal grafico notiamo che abbiamo collassato i punti in cui deve essere falso nel punto $(0,0)$, e possiamo dividere linearmente!

In effetti non possiamo fare learning in questo caso come abbiamo visto nel capitolo scorso, perchè non possiamo sistemare i pesi per risolvere questo tipo di network che può risolvere problemi più complessi, non sappiamo a priori quali dovrebbero essere i pesi del livello intermedio, ma lo risolveremo con la backpropagation.



## 2  Regressione

La regressione è una tecnica di learning supervisionato dove data una funzione parametrizzata $f:R^n\rightarrow R^{n'}$ cerca di approssimare la relazione tra input e output.

Immaginiamo di avere:
$$
Y_{test} = \{y^{(i)}\}_{i=1}^m \\
\hat Y = \{f_\theta (x^{(i)}) \}_{i=1}^m
$$
dove $m$ è il numero di dati che abbiamo nel dataset. Si intende quindi che l'$i$-esimo campione avrà struttura $Y^{(i)}=y_1^{(i)},...,y_{n'}^{(i)}$. 

Possiamo valutare la regressione lineare utilizzando il Root Mean Square Error, cioè una distanza euclidea.
$$
RMSE = \sqrt{\sum_{j=1}^{n'}(\hat y^{(i)}_j - y^{(i)}_j)^2}
$$
E di conseguenza l'errore:
$$
Error(Y_{test}, \hat Y_{test}) = \frac1m \sum_{i=1}^m{||\hat y^{(i)} - y^{(i)}||_2}
$$
Volendo potremmo anche togliere la radice quadrata, peggiorando diciamo l'importanza gli errori che possono nascere.
$$
MSE = \sum_{j=1}^{n'}\big (\hat y^{(i)}_j - y^{(i)}_j\big )^2
$$
Abbiamo definito la regressione come una funzione parametrizzata $f:R^n \rightarrow R^{n'}$, ma ci sono tre casi specifici:

1. $n=n'=1$ :arrow_right: Simple Regression quindi $f_\theta(x)=y$, con $\theta_0+x_1+\theta_1$
2. $n>1$, $n'=1$ :arrow_right: Multiple Regression quindi $f_\theta(x_n) = y$ con $\theta_0 + x_1\theta_1+...+x_n\theta_n = \vec\theta^T\vec x$.
3. $n>1$, $n'>1$ :arrow_right: Multivariate Regression avremo sostanzialmente $n$ regressioni lineari ognuna di esse darà un output

Definiamo la **Loss Function**, prende in input il vettore dei parametri e deve stimare l'errore sui dati di training
$$
J(v) = \frac12 \sum_{i=1}^m \big (f_v(x^{(i)})-y^{(i)}\big )
$$
Di tale funzione vogliamo trovare il minimo, quindi trovare i parametri che ci consentono tale situazione. 

Ad ogni iterazione cambieremo i parametri, facendo scendere la quantità degli errori. Per farlo, mando un input random e calcolo la derivata, se è negativa significa che mi conviene aggiungere qualcosa, viceversa se positiva significa che mi conviene togliere qualcosa. 

Tale algoritmo si chiama discesa del gradiente. Via via raggiungeremo il punto di zero, che sarà il punto di minimo della funzione. Possiamo impostare anche un parametro detto learning rate che ci permette di velocizzare il learning.g
$$
v^t_j =v_j^{t-1} - \gamma \frac {\partial \delta  } {\partial v_j }
$$
Se i dati hanno scale molto diverse la discesa del gradiente potrebbe far fatica per raggiungere il minimo, quindi meglio normalizzare. La funzione di costo può avere una forma che rende difficile la convergenza. Normalizzare le feature, ad esempio tra 0 e 1, può rendere più facile e veloce raggiungere il minimo. Successivamente selezioniamo una split per training e validation, fissiamola e trainiamo. Dopo di che confrontiamo i modelli (variano gli iperparametri) e così li selezioniamo.

### 2.1  REC Curve

La REC Curve è un metodo grafico per valutare i modelli di regressione. Cioè che facciamo è:

1. Per ogni campione del test set calcoliamo l’errore tra la predizione e la ground truth.
2. Ordiniamo gli errori in ordine crescente
3. Costruiamo il grafico dove sull’asse x rappresentiamo la tolleranza all’errore $\epsilon_i'$, è un valore arbitrario e dipende da quanto siamo disposti a tollerare un errore, mentre l’asse y mostra la percentuale cumulativa di punti predetti nella tolleranza corrispodente
4. Tracciamo il grafico come definito al punto 3, avremo una curva monotona crescente che parte da $(0,0)$ e arriva a $(\epsilon_i,1)$ sull’errore massimo.

Una REC curve ideale va rapidamente a 1 perchè gli errori sono piccoli. L’area sotto la curva fornisce una misura della bontà dell’algoritmo. Consiste in un buon metodo per confrontare modelli di regressione che hanno errori medi simili, ma preferiamo vedere come si distribuiscono. Meglio quello con errori più piccoli.



### 2.2  Equazione Normale

La discesa del gradiente trova i parametri che minimizzano la loss function in maniera iterativa, invece l’equazione normale trova i parametri in un solo stem risolvendo il problema analiticamente

![image-20250428153705569](C:\Users\simon\AppData\Roaming\Typora\typora-user-images\image-20250428153705569.png)

Abbiamo un training set T e un vettore di output Y, l’equazione normale fornisce una formula diretta per il calcolo dei parametri. Sostanzialmente è:
$$
\hat \theta = \min_\theta J(\theta) = (T^TT)^{-1}T^Ty
$$
Calcolare questa equazione è un’operazione che dipende dai parametri dato che ha una complessità $O(n^3)$ pertanto per n piccoli va bene altrimenti usiamo la discesa del gradiente.



### 2.3  Capacità del modello, overfitting e underfitting

Più un modello ha parametri, più è complesso e più è capace. Maggior capacità non implica migliori risultati generali perchè potremmo finire per fittare perfettamente i dati di training con curve davvero complesse ma finire per avere risultati scadenti in fase di testing. Questa situazione viene detta **overfitting**, e dato che c’è un grosso salto tra errori su training set e test set si dice che c’è **alta varianza**.

Invece potrebbe succedere l’opposto, se un modello è troppo semplice allora non fitta bene nè i dati di training nè quelli di testing, cioè è una situazione di underfitting, e si dice che c’è un bias alto.

![image-20250428162814760](C:\Users\simon\AppData\Roaming\Typora\typora-user-images\image-20250428162814760.png)

Con underfitting aumentare il dataset ovviamente non aiuta a risolvere il problema, serve aumentare la sua complessità. Invece con l’overfitting potrebbe essere utile aumentare i dati, modelli molto complessi quindi hanno bisogno di molti dati (Vi dice niente riguardo GPT?). Può aiutare anche ridurre la complessità del modello o regolarizzare i dati.

La regolarizzazione consiste nel migliorare la capacità del modello per permettergli di generalizzare su dati mai visti, evidando di adattarsi troppo al training set. La funzione di loss che misura l’errore viene modificata aggiungendo un termine di penalizzazione, progettato per penalizzare valori elevati dei parametri del modello e prediligendo quindi valori più piccoli.
$$
J_{reg}(\theta) = J(\theta) + \lambda\sum_{j=1}^d \theta_j^2
$$
dove $\lambda$ è un iperparametro che controlla l’intensità della regolarizzazione. Non si include il parametro $\theta_0$ ovvero il bias o intercetta perchè è quello ce consente di spostare l’intercetta del modello.





## 3  Classificazione

La classificazione consiste nel partizionare lo spazio delle features in regioni distinte, ciascuna delle quali rappresenta una classe predefinita. Tali regioni si delineano con confini decisionali. Distinguiamo classificazione binaria e multiclasse. Nella classificazione binaria vogliamo dividere lo spazio in due regioni, non è bene fare regressione lineare perchè non fitta bene i dati. Meglio usare una regressione logistica:



![image-20250428171243292](C:\Users\simon\AppData\Roaming\Typora\typora-user-images\image-20250428171243292.png)



L’output del modello si interpreta come la probabilità di assegnare il dato alla classe positiva. Si calcola mediante la funzione digmoide

![image-20250428171359457](C:\Users\simon\AppData\Roaming\Typora\typora-user-images\image-20250428171359457.png)

Tale equazione descrive un iperpiano che separa le due classi. Il confine consiste nella probabilità del 50%. Potremmo decidere confini più complessi come quelli non lineari. La funzione sigma è detta sigmoide, possiamo scriverla usando la tangente iperbolica.

La Cross-Entropy loss è fondamentale per la classificazione binaria, consiste in:

![image-20250428181912760](C:\Users\simon\AppData\Roaming\Typora\typora-user-images\image-20250428181912760.png)

dove $h_\theta(x)$ è la probabilità predetta mentre $y^{(i)}$ la ground truth. Misura quanto le previsioni del modello si discostano dalle etichette reali. In fase di training vogliamo minimizzare tale funzione.

Inoltre è possibile definire delle zone di rigetto, ovvero quelle aree dubbie dove il modello non è in grado di assegnare facilmente una classe all’input, pertanto piuttosto che sbagliare preferisce rifiutare di rispondere.

Quando invece abbiamo classificazione multiclasse vogliamo trovare più di due regioni, possiamo risolvere questo problema con più classificatori binari. Le etichette sono rappresentate mediante one-hot encoding.

- OvA: Si addestra un classificatore binario per ogni classe, ognuno ha l’obiettivo di distinguere una classe dalle altre combinate (considerate tutte negative). Per esempio con tre classi A, B e C avremo un classificatore che distingue elementi di classe A da elementi di classe non A e così via per le altre.
  <img src="C:\Users\simon\AppData\Roaming\Typora\typora-user-images\image-20250428182824299.png" alt="image-20250428182824299" style="zoom: 50%;" />
- OvO: si addestra un classificatore binario per ogni coppia di classi, quindi se abbiamo K classi addestreremo $\frac{K(K-1)}{2}$. Alla fine si sceglie quello che ha più vittorie.
  <img src="C:\Users\simon\AppData\Roaming\Typora\typora-user-images\image-20250428183205091.png" alt="image-20250428183205091" style="zoom:50%;" />

In caso di pareggi andiamo a considerare le distanze. Si possono creare anche zone dove ogni probabilità è sotto il 50% per ogni classe.



### 3.1 Softmax

La regressione logistica consiste in un framework di learning utile a distinguere due classi. Si utilizza, come già anticipato, la funzione sigmoide .
$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
Il training set ovviamente è etichettato dato che siamo nel supervised learning. Usiamo come loss function la Cross-Entropy Loss:
$$
J(\theta)=-\frac1m\Big [ \sum_{i=1}^m y^{(i)}\log\big(h_\theta(x^{(i)})\big)+(1-y^{(i)})\log\big(1-h_\theta(x^{(i)})\big) \Big]
$$
E utilizziamo la discesa del gradiente per trovare i parametri, ripetendo per ogni $j$ simultaneamente l’operazione:
$$
\theta_j \leftarrow \theta_j-\alpha\frac{\partial J(\theta)}{\partial \theta_j}
$$
Usando come learning rate valori come $0.1,0.003, 0.001 …$

Per prevenire l’overfitting ci serviamo della regolarizzazione, e per gestire la classificazione multiclasse ci serviamo dell’approccio OvA.

Il Softmax è un framework di learning per fare proprio la multiclass classification ed è una generalizzazione della regressione logistica. Per un dato input vogliamo stimare la probabilità che l’output associato sia $k$, per ogni $k$ nell’insieme degli output possibili. Cioè:
$$
P_{\theta^{(k)}}(y=k|x)\ \ \ \ \ \forall k=0,1,..., k-1
$$
Il modello softmax si descrive come:







Per una data classe, possiamo riassumerlo con:



L’obiettivo del learning ovviamente è  trovarere i parametri e usiamo sempre la discesa del gradiente. Solo che softmax ha la caratteristica di essere overparametrizzato, nel senso che ci sono molti setting di parametri che danno la stessa soluzione.

Quindi i componenti del softmax sono:



Ognuno di essi ha un






































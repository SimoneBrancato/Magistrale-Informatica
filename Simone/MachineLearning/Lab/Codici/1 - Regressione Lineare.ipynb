{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vediamo come costruire e allenare un regressore lineare con PyTorch. Carichiamo il dataset Boston con scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\simon\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\simon\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\simon\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\simon\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "Variables in order:\n",
      "CRIM     per capita crime rate by town\n",
      "ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "INDUS    proportion of non-retail business acres per town\n",
      "CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "NOX      nitric oxides concentration (parts per 10 million)\n",
      "RM       average number of rooms per dwelling\n",
      "AGE      proportion of owner-occupied units built prior to 1940\n",
      "DIS      weighted distances to five Boston employment centres\n",
      "RAD      index of accessibility to radial highways\n",
      "TAX      full-value property-tax rate per $10,000\n",
      "PTRATIO  pupil-teacher ratio by town\n",
      "B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "LSTAT    % lower status of the population\n",
      "MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "\n",
      "Information about the dataset\n",
      "CLASSTYPE: numeric\n",
      "CLASSINDEX: last\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "boston = fetch_openml(name=\"boston\", version=1)\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset consiste in 506 osservazioni, ognuna di esse ha 13 valori che costituiscono rilevamenti di vicinati di Boston.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data.to_numpy().astype(float)\n",
    "Y = boston.target.to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X sono i rilevamenti, quindi sono 506 quartieri con ognuno contenente 13 rilevamenti\n",
    "Y sono i valori mediani delle case in ognuno dei vicinati espressi in multipli di 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo una permutazione casuale degli indici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.random.manual_seed(123)\n",
    "\n",
    "idx = np.random.permutation(len(X))\n",
    "\n",
    "# Applichiamo la stessa permutazione a rilevamenti e target\n",
    "X = X[idx]\n",
    "Y = Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adesso suddividiamo il dataset in training e testing selezionando i primi 50 valori per formare il testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trasformiamo gli array in tensori e inseriamoli in delle variabili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = torch.Tensor(X[50:])\n",
    "Y_training = torch.Tensor(Y[50:])\n",
    "X_testing = torch.Tensor(X[:50])\n",
    "Y_testing = torch.Tensor(Y[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Perchè abbiamo effetuato una permutazione casuale dei dati prima di dividerli in training e test?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perchè così se il dataset ha un bias andiamo a renderlo omogeneo e dividiamo correttamente tra train e test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello di regressione dipende da 14 parametri:\n",
    "- 13 sono le features in ingresso\n",
    "- 1 l'intercetta della retta\n",
    "Iniializziamo l'array dei pesi $\\theta$ casualmente con distribuzione normale media zero varianza 0.1 così che inizialmente abbia valori sia positivi che negativi \n",
    "\n",
    "Definiamo una variabile per l'intercetta, ovvero theta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il bias in un modello di regressione lineare è l'intercetta della retta, ovvero il valore che la funzione di regressione assume quando tutte le variabili indipendenti sono pari a zero.\n",
    "\n",
    "La funzione del modello è:\n",
    "$$\n",
    "y(x) = \\theta_1x_1+...+\\theta_{13}x_{13}+\\theta_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0111,  0.0120, -0.0370, -0.0240, -0.1197,  0.0209, -0.0972, -0.0755,\n",
      "         0.0324, -0.0109,  0.0210, -0.0391,  0.0235], requires_grad=True)\n",
      "tensor([0.0665], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "theta = torch.Tensor(13) # Tensore da 13 unità\n",
    "theta_0 = torch.Tensor(1) # Bias\n",
    "\n",
    "theta.requires_grad_(True)\n",
    "theta_0.requires_grad_(True)\n",
    "\n",
    "theta.data.normal_(0,0.1)\n",
    "theta_0.data.normal_(0,0.1)\n",
    "\n",
    "print(theta)\n",
    "print(theta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, l'attributo requires_grad è fondamentale per il calcolo automatico del gradiente e l'ottimizzazione dei modelli di machine learning.\n",
    "\n",
    "Quando un tensore ha requires_grad=True, PyTorch tiene traccia delle operazioni fatte su di esso, costruendo un grafo computazionale dinamico. Questo permette di calcolare automaticamente i gradienti tramite backpropagation, essenziali per l'ottimizzazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(input, theta, theta_0):\n",
    "    return input.mul(theta).sum(1)+theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.2912e-04,  4.8145e-01, -2.3694e-01,  ...,  3.7018e-01,\n",
      "         -1.5513e+01,  8.2946e-02],\n",
      "        [-6.7404e-04,  0.0000e+00, -9.0930e-02,  ...,  3.7439e-01,\n",
      "         -1.5130e+01,  3.0899e-01],\n",
      "        [-7.0198e-03,  0.0000e+00, -3.0088e-01,  ...,  4.4170e-01,\n",
      "         -1.5513e+01,  1.9409e-01],\n",
      "        ...,\n",
      "        [-3.9141e-03,  0.0000e+00, -2.7279e-01,  ...,  4.1225e-01,\n",
      "         -1.5513e+01,  1.8093e-01],\n",
      "        [-1.0241e-01,  0.0000e+00, -6.6904e-01,  ...,  4.2487e-01,\n",
      "         -1.5513e+01,  5.5454e-01],\n",
      "        [-5.0783e-02,  0.0000e+00, -6.6904e-01,  ...,  4.2487e-01,\n",
      "         -1.3863e+01,  1.6730e-01]], grad_fn=<MulBackward0>) torch.Size([456, 13])\n"
     ]
    }
   ],
   "source": [
    "res = X_training.mul(theta)\n",
    "print(res, res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo moltiplicato ogni riga di X_training per il vettore theta, quindi stiamo formando l'equazione di regressione lineare. Infine, sommiamo lungo l'asse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo restituisce per ogni campione la combinazione lineare, adesso manca solo theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = X_training.mul(theta).sum(1)+theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-20.8190, -23.3360, -24.6247, -31.4600, -27.4998, -28.1950, -27.1047,\n",
      "        -26.2724, -18.8539, -31.4594], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = linear_regression(X_training, theta, theta_0)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confrontiamo questi valori con i veri valori quindi Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32.4000, 29.6000, 20.4000, 12.3000, 19.1000, 14.9000, 17.8000,  8.8000,\n",
      "        35.4000, 11.5000])\n"
     ]
    }
   ],
   "source": [
    "print(Y_training[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori sono molto diversi ed è normale perchè il modello per ora usa pesi inizializzati a caso, adesso dobbiamo ottimizzare il modello tramite la discesa del gradiente. Per farlo definiamo una funzione di loss cioè una funzione differenziabile che esprime l'errore che il modello commette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui useremo Mean Squared Error.\n",
    "$$\n",
    "MSE(y, \\hat y) = \\frac 1N \\sum_i^N(\\hat y_i - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dove $\\hat y$ sono le predizioni, mentre $y$ sono le ground truth e N il numero di campioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(input, target):\n",
    "    return ((input-target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2274.0801, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss(y,Y_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è un numero molto grande perchè le predizioni sono molto distanti dalle GT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

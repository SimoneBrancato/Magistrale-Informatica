{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementazione GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduzione alle Graph Attention Networks (GAT)\n",
    "\n",
    "Le **Graph Attention Networks (GAT)** sono un tipo di rete neurale progettata per operare su **dati strutturati come grafi**, combinando la flessibilità delle reti neurali con la potenza del **meccanismo di attenzione**.\n",
    "\n",
    "A differenza dei modelli tradizionali come le GCN (Graph Convolutional Networks), le GAT assegnano **pesi diversi** ai nodi vicini durante l'aggregazione, apprendendo **quali vicini sono più rilevanti** per ogni nodo. Questo è possibile grazie a un meccanismo di **self-attention** che calcola coefficienti di attenzione tra i nodi connessi.\n",
    "\n",
    "### Vantaggi delle GAT:\n",
    "- **Importanza adattiva dei vicini**\n",
    "- **Robustezza al rumore nei dati**\n",
    "- **Applicabilità a grafi non regolari**\n",
    "- **Efficienza e parallelizzabilità**\n",
    "\n",
    "Le GAT si usano in vari ambiti, come social network, biologia computazionale, knowledge graph e sistemi di raccomandazione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struttura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple PyTorch Implementation of the Graph Attention layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GATLayer, self).__init__()\n",
    "      \n",
    "    def forward(self, input, adj):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trasformazione lineare\n",
    "\n",
    "$$\n",
    "\\bar{h'}_i = \\textbf{W}\\cdot \\bar{h}_i\n",
    "$$\n",
    "with $\\textbf{W}\\in\\mathbb R^{F'\\times F}$ and $\\bar{h}_i\\in\\mathbb R^{F}$.\n",
    "\n",
    "$$\n",
    "\\bar{h'}_i \\in \\mathbb{R}^{F'}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice W dimensione: torch.Size([5, 2])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2402, -1.2035],\n",
      "        [ 0.2408, -0.0326],\n",
      "        [-0.1906,  0.3131],\n",
      "        [ 0.8759, -0.2736],\n",
      "        [ 0.6560,  0.8273]], requires_grad=True) \n",
      "\n",
      "\n",
      "Dati di input: torch.Size([3, 5])\n",
      "tensor([[0.0326, 0.6763, 0.8810, 0.5566, 0.4229],\n",
      "        [0.3134, 0.2122, 0.7162, 0.6117, 0.2799],\n",
      "        [0.3356, 0.2422, 0.9516, 0.6698, 0.4737]]) \n",
      "\n",
      "\n",
      "Trasformazione h': torch.Size([3, 2])\n",
      "tensor([[ 0.7677,  0.4122],\n",
      "        [ 0.7092, -0.0957],\n",
      "        [ 0.8549,  0.0948]], grad_fn=<MmBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_features = 5\n",
    "out_features = 2\n",
    "nb_nodes = 3\n",
    "\n",
    "W = nn.Parameter(torch.zeros(size=(in_features, out_features))) #xavier paramiter inizializator\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "\n",
    "#Xavier Parameter Initialization (o Glorot initialization) è una tecnica per inizializzare i pesi di una rete neurale in modo intelligente.\n",
    "#Problema che risolve:\n",
    "# - Se i pesi sono troppo piccoli → il gradiente \"svanisce\" durante la backpropagation\n",
    "# - Se i pesi sono troppi grandi → il gradiente \"esplode\" e il training diventa instabile\n",
    "#Obiettivo:\n",
    "# Mantenere la varianza dei segnali (attivazioni e gradienti) costante attraverso tutti i layer, evitando che si ampllifichino o si riducano troppo.\n",
    "\n",
    "\n",
    "print(\"\\nMatrice W dimensione:\",W.shape)\n",
    "print(W,\"\\n\")\n",
    "\n",
    "input = torch.rand(nb_nodes,in_features) \n",
    "print(\"\\nDati di input:\",input.shape)\n",
    "print(input,\"\\n\")\n",
    "\n",
    "# linear transformation\n",
    "h = torch.mm(input, W)\n",
    "print(\"\\nTrasformazione h':\",h.shape)\n",
    "print(h,\"\\n\")\n",
    "N = h.size()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meccanismo di attenzione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](AttentionMechanism.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "tensor([[ 0.4327],\n",
      "        [-0.7629],\n",
      "        [-0.2371],\n",
      "        [ 0.3441]])\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))  # Crea un parametro trainabile inizializzato a zero\n",
    "# Dimensione (2*out_features, 1): vettore colonna per il meccanismo di attenzione GAT\n",
    "# Il fattore 2 deriva dalla concatenazione delle feature di due nodi (h_i || h_j)\n",
    "# nn.Parameter() rende questo tensor un parametro del modello (verrà aggiornato durante il training)\n",
    "\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)  # Applica inizializzazione Xavier uniforme\n",
    "# xavier_uniform_ sostituisce i valori zero con valori casuali dalla distribuzione uniforme\n",
    "# gain=1.414 ≈ sqrt(2) è il guadagno raccomandato per LeakyReLU/ELU\n",
    "# Il gain compensa il fatto che LeakyReLU riduce la varianza dei valori negativi\n",
    "\n",
    "print(a.shape)  # Stampa la forma del tensor: dovrebbe essere (2*out_features, 1)\n",
    "print(a.data)   # Stampa i valori inizializzati con Xavier (numeri casuali piccoli e bilanciati)\n",
    "\n",
    "leakyrelu = nn.LeakyReLU(0.2)  # Crea funzione di attivazione LeakyReLU con pendenza 0.2\n",
    "# Per valori positivi: f(x) = x\n",
    "# Per valori negativi: f(x) = 0.2 * x (invece di 0 come in ReLU normale)\n",
    "# Questo evita il \"dying ReLU problem\" mantenendo un piccolo gradiente per valori negativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7677,  0.4122,  0.7677,  0.4122],\n",
       "         [ 0.7677,  0.4122,  0.7092, -0.0957],\n",
       "         [ 0.7677,  0.4122,  0.8549,  0.0948]],\n",
       "\n",
       "        [[ 0.7092, -0.0957,  0.7677,  0.4122],\n",
       "         [ 0.7092, -0.0957,  0.7092, -0.0957],\n",
       "         [ 0.7092, -0.0957,  0.8549,  0.0948]],\n",
       "\n",
       "        [[ 0.8549,  0.0948,  0.7677,  0.4122],\n",
       "         [ 0.8549,  0.0948,  0.7092, -0.0957],\n",
       "         [ 0.8549,  0.0948,  0.8549,  0.0948]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features)\n",
    "# Questa riga costruisce tutte le possibili coppie di nodi per il calcolo dell'attenzione\n",
    "\n",
    "# SCOMPOSIZIONE PASSO PER PASSO:\n",
    "\n",
    "# 1. h.repeat(1, N) \n",
    "# Ripete ogni riga di h per N volte orizzontalmente\n",
    "# Se h è (3, 2) e N=3: [[a,b], [c,d], [e,f]] → [[a,b,a,b,a,b], [c,d,c,d,c,d], [e,f,e,f,e,f]]\n",
    "\n",
    "# 2. h.repeat(1, N).view(N * N, -1)\n",
    "# Reshape per ottenere N*N righe, ogni riga rappresenta h_i replicato\n",
    "# Risultato: (N*N, out_features) dove ogni blocco di N righe contiene lo stesso nodo\n",
    "\n",
    "# 3. h.repeat(N, 1)\n",
    "# Ripete l'intera matrice h per N volte verticalmente  \n",
    "# Se h è (3, 2): [[a,b], [c,d], [e,f]] → [[a,b], [c,d], [e,f], [a,b], [c,d], [e,f], [a,b], [c,d], [e,f]]\n",
    "# Risultato: (N*N, out_features) dove i nodi si ripetono ciclicamente\n",
    "\n",
    "# 4. torch.cat([...], dim=1)\n",
    "# Concatena orizzontalmente le due matrici precedenti\n",
    "# Ogni riga contiene [h_i, h_j] per una specifica coppia (i,j)\n",
    "# Risultato: (N*N, 2*out_features)\n",
    "\n",
    "# 5. .view(N, -1, 2 * out_features)\n",
    "# Reshape finale per ottenere (N, N, 2*out_features)\n",
    "# a_input[i][j] contiene la concatenazione [h_i, h_j]\n",
    "\n",
    "a_input\n",
    "# Il tensor finale rappresenta TUTTE le possibili coppie di nodi del grafo\n",
    "# Dimensioni: (N, N, 2*out_features)\n",
    "# a_input[i][j] = concatenazione delle feature del nodo i e del nodo j\n",
    "# Questo permette al meccanismo di attenzione di calcolare quanto il nodo i dovrebbe \"prestare attenzione\" al nodo j\n",
    "\n",
    "# ESEMPIO PRATICO:\n",
    "# Se abbiamo 3 nodi con feature [h1, h2, h3]\n",
    "# a_input conterrà:\n",
    "# [[h1||h1, h1||h2, h1||h3],\n",
    "#  [h2||h1, h2||h2, h2||h3], \n",
    "#  [h3||h1, h3||h2, h3||h3]]\n",
    "# dove || indica concatenazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](a_input.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0045, -0.0367, -0.0305],\n",
      "        [ 0.3397,  0.1788,  0.2098],\n",
      "        [ 0.2574,  0.0965,  0.1275]], grad_fn=<LeakyReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4]) torch.Size([4, 1])\n",
      "\n",
      "torch.Size([3, 3, 1])\n",
      "\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(a_input.shape,a.shape)\n",
    "print(\"\")\n",
    "print(torch.matmul(a_input,a).shape)\n",
    "print(\"\")\n",
    "print(torch.matmul(a_input,a).squeeze(2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mascheriamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [1, 1, 1]])\n",
      "torch.Size([3, 3])\n",
      "tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n"
     ]
    }
   ],
   "source": [
    "# Masked Attention\n",
    "adj = torch.randint(2, (3, 3))\n",
    "print(adj)\n",
    "\n",
    "zero_vec  = -9e15*torch.ones_like(e)\n",
    "print(zero_vec.shape)\n",
    "print(zero_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [1, 1, 1]]) \n",
      " tensor([[-0.0045, -0.0367, -0.0305],\n",
      "        [ 0.3397,  0.1788,  0.2098],\n",
      "        [ 0.2574,  0.0965,  0.1275]], grad_fn=<LeakyReluBackward0>) \n",
      " tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-9.0000e+15, -3.6663e-02, -9.0000e+15],\n",
       "        [-9.0000e+15, -9.0000e+15,  2.0982e-01],\n",
       "        [ 2.5738e-01,  9.6488e-02,  1.2751e-01]], grad_fn=<WhereBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = torch.where(adj > 0, e, zero_vec)\n",
    "print(adj,\"\\n\",e,\"\\n\",zero_vec)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7677,  0.4122],\n",
      "        [ 0.7092, -0.0957],\n",
      "        [ 0.8549,  0.0948]], grad_fn=<MmBackward0>)\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "attention = F.softmax(attention, dim=1)\n",
    "h_prime   = torch.matmul(attention, h)\n",
    "print(h)\n",
    "print(h.shape)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0000],\n",
       "        [0.3664, 0.3119, 0.3217]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7092, -0.0957],\n",
       "        [ 0.8549,  0.0948],\n",
       "        [ 0.7775,  0.1517]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h_prime vs h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7092, -0.0957],\n",
      "        [ 0.8549,  0.0948],\n",
      "        [ 0.7775,  0.1517]], grad_fn=<MmBackward0>) \n",
      " tensor([[ 0.7677,  0.4122],\n",
      "        [ 0.7092, -0.0957],\n",
      "        [ 0.8549,  0.0948]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(h_prime,\"\\n\",h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costruiamo il layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        TODO\n",
    "        '''\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        # Linear Transformation\n",
    "        h = torch.mm(input, self.W)  # Trasformazione lineare: moltiplica le feature di input per la matrice dei pesi W\n",
    "        N = h.size()[0]  # Ottiene il numero di nodi nel grafo\n",
    "    \n",
    "        # Attention Mechanism\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        # Crea tutte le possibili coppie di nodi concatenando h_i e h_j per ogni coppia (i,j)\n",
    "        # h.repeat(1, N) ripete ogni riga N volte, h.repeat(N, 1) ripete l'intera matrice N volte\n",
    "        # Il risultato è un tensor (N, N, 2*out_features) con le feature concatenate di ogni coppia\n",
    "    \n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        # Calcola i coefficienti di attenzione grezzi applicando la trasformazione lineare 'a'\n",
    "        # seguita da LeakyReLU come funzione di attivazione\n",
    "    \n",
    "        # Masked Attention\n",
    "        zero_vec = -9e15*torch.ones_like(e)  # Crea un tensor di valori molto negativi con la stessa forma di e\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        # Applica la maschera della matrice di adiacenza: mantiene i valori di e dove esiste un arco (adj > 0),\n",
    "        # altrimenti imposta valori molto negativi per escludere connessioni inesistenti\n",
    "    \n",
    "        attention = F.softmax(attention, dim=1)  # Applica softmax per normalizzare i pesi di attenzione lungo ogni riga\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)  # Applica dropout per regolarizzazione durante il training\n",
    "        h_prime = torch.matmul(attention, h)  # Calcola le nuove rappresentazioni dei nodi come combinazione pesata dei vicini\n",
    "    \n",
    "        if self.concat:  # Se questo layer deve concatenare i risultati di più attention head\n",
    "            return F.elu(h_prime)  # Applica ELU come funzione di attivazione\n",
    "        else:  # Se questo è l'ultimo layer o non serve concatenazione\n",
    "            return h_prime  # Ritorna direttamente le nuove rappresentazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GATLayer, self).__init__()  # Chiama il costruttore della classe padre nn.Module\n",
    "        self.dropout       = dropout        # Probabilità di dropout (es. 0.6) per regolarizzazione\n",
    "        self.in_features   = in_features    # Numero di feature in input per ogni nodo\n",
    "        self.out_features  = out_features   # Numero di feature in output per ogni nodo\n",
    "        self.alpha         = alpha          # Parametro per LeakyReLU (pendenza per valori negativi, es. 0.2)\n",
    "        self.concat        = concat         # True per tutti i layer tranne l'ultimo (concatena multi-head)\n",
    "        \n",
    "        # Xavier Initialization of Weights\n",
    "        # Alternativa: usare weights_init per applicare pesi personalizzati\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        # Matrice dei pesi W per la trasformazione lineare delle feature dei nodi\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        # Inizializzazione Xavier uniforme per stabilizzare il training\n",
    "        \n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        # Vettore di parametri per il meccanismo di attenzione (dimensione doppia per concatenazione)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        # Inizializzazione Xavier per i parametri di attenzione\n",
    "        \n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        # Funzione di attivazione LeakyReLU con pendenza alpha per valori negativi\n",
    "    \n",
    "    def forward(self, input, adj):\n",
    "        # Linear Transformation\n",
    "        h = torch.mm(input, self.W)  # Trasformazione lineare: moltiplica i features di input per la matrice dei pesi W\n",
    "        N = h.size()[0]  # Ottiene il numero di nodi nel grafo\n",
    "\n",
    "\n",
    "        # Attention Mechanism\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        # Crea tutte le possibili coppie di nodi concatenando h_i e h_j per ogni coppia (i,j)\n",
    "        # h.repeat(1, N) ripete ogni riga N volte, h.repeat(N, 1) ripete l'intera matrice N volte\n",
    "        # Il risultato è un tensor (N, N, 2*out_features) con le feature concatenate di ogni coppia\n",
    "        \n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        # Calcola i coefficienti di attenzione grezzi applicando la trasformazione lineare 'a'\n",
    "        # seguita da LeakyReLU come funzione di attivazione, squeeze rimuove la dimensione singleton\n",
    "        \n",
    "        # Masked Attention\n",
    "        zero_vec = -9e15*torch.ones_like(e)  # Crea un tensor di valori molto negativi con la stessa forma di e\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        # Applica la maschera della matrice di adiacenza: mantiene i valori di e dove esiste un arco (adj > 0),\n",
    "        # altrimenti imposta valori molto negativi per escludere connessioni inesistenti dal softmax\n",
    "        \n",
    "        attention = F.softmax(attention, dim=1)  # Applica softmax per normalizzare i pesi di attenzione lungo ogni riga\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)  # Applica dropout per regolarizzazione durante il training\n",
    "        h_prime = torch.matmul(attention, h)  # Calcola le nuove rappresentazioni dei nodi come combinazione pesata dei vicini\n",
    "        \n",
    "        if self.concat:  # Se questo layer deve concatenare i risultati di più head attention\n",
    "            return F.elu(h_prime)  # Applica ELU come funzione di attivazione per layer intermedi\n",
    "        else:  # Se questo è l'ultimo layer o non serve concatenazione\n",
    "            return h_prime  # Ritorna direttamente le nuove rappresentazioni senza attivazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione di attivazione ELU (Exponential Linear Unit)\n",
    "\n",
    "La funzione ELU è definita come:\n",
    "$$\n",
    "\\text{ELU}(x) = \\begin{cases} x & \\text{se} x > 0 \\\\\\\\ \\alpha (e^x - 1) & \\text{se } x \\leq 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Dove:\n",
    "- $x$ è l'input del neurone,\n",
    "- $\\alpha \\in \\mathbb{R}^+ $ è un iperparametro (solitamente $\\alpha = 1.0 $).\n",
    "\n",
    "### Derivata della funzione ELU:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\text{ELU}(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{se } x > 0 \\\\\\\\\n",
    "\\text{ELU}(x) + \\alpha & \\text{se } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Proprietà:\n",
    "- Continuità e derivabilità ovunque\n",
    "- Riduce il rischio di neuroni morti\n",
    "- Media delle attivazioni centrata intorno a zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usiamolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes in Cora: 7\n",
      "Number of Node Features in Cora: 1433\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "name_data = 'Cora'\n",
    "dataset = Planetoid(root= '/tmp/' + name_data, name = name_data)\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "\n",
    "print(f\"Number of Classes in {name_data}:\", dataset.num_classes)\n",
    "print(f\"Number of Node Features in {name_data}:\", dataset.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9461, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2426, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0849, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0031, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0685, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()  # Chiama il costruttore della classe padre nn.Module\n",
    "        self.hid = 16  # Numero di feature nascoste nel layer intermedio\n",
    "        self.in_head = 16  # Numero di head attention nel primo layer\n",
    "        self.out_head = 1  # Numero di head attention nel layer di output\n",
    "        \n",
    "        self.conv1 = GATConv(dataset.num_features, self.hid, concat=False, heads=self.in_head, dropout=0.6)\n",
    "        # Primo layer GAT: trasforma da num_features del dataset a hid feature nascoste\n",
    "        # concat=False significa che le head multiple vengono mediate invece che concatenate\n",
    "        # heads=in_head usa 16 head attention parallele\n",
    "        # dropout=0.6 applica dropout con probabilità 60%\n",
    "        \n",
    "        self.conv2 = GATConv(self.hid  # *self.in_head (commentato perché concat=False)\n",
    "                             , dataset.num_classes, concat=False,\n",
    "                             heads=self.out_head, dropout=0.6)\n",
    "        # Secondo layer GAT: trasforma da hid feature nascoste al numero di classi\n",
    "        # Una sola head attention per l'output finale\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index  # Estrae feature dei nodi e indici degli archi\n",
    "                \n",
    "        x = F.dropout(x, p=0.6, training=self.training)  # Applica dropout alle feature di input durante il training\n",
    "        x = self.conv1(x, edge_index)  # Primo layer GAT con multi-head attention\n",
    "        x = F.elu(x)  # Applica funzione di attivazione ELU\n",
    "        x = F.dropout(x, p=0.6, training=self.training)  # Dropout tra i layer\n",
    "        x = self.conv2(x, edge_index)  # Secondo layer GAT per la classificazione finale\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)  # Applica log-softmax per ottenere probabilità logaritmiche\n",
    "    \n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Seleziona GPU se disponibile, altrimenti CPU\n",
    "model = GAT().to(device)  # Crea il modello GAT e lo sposta sul device selezionato\n",
    "data = dataset[0].to(device)  # Carica il primo grafo dal dataset e lo sposta sul device\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Ottimizzatore Adam con learning rate 0.01 e weight decay per regolarizzazione\n",
    "model.train()  # Imposta il modello in modalità training\n",
    "\n",
    "for epoch in range(1000):  # Loop di training per 3000 epoche\n",
    "    model.train()  # Assicura che il modello sia in modalità training\n",
    "    optimizer.zero_grad()  # Azzera i gradienti accumulati dall'iterazione precedente\n",
    "    out = model(data)  # Forward pass: calcola le predizioni del modello\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])  # Calcola la negative log likelihood loss solo sui nodi di training\n",
    "    \n",
    "    if epoch % 200 == 0:  # Ogni 200 epoche\n",
    "        print(loss)  # Stampa il valore della loss per monitorare il progresso\n",
    "    \n",
    "    loss.backward()  # Backpropagation: calcola i gradienti\n",
    "    optimizer.step()  # Aggiorna i parametri del modello usando i gradienti\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# Machine Learning

[TOC]

## 1  Introduzione

Il machine learning consiste in quel campo di studi che permette ai computer di imparare senza essere esplicitamente programmati. I relativi algoritmi sono in grado di apprendere come risolvere uno specifico problema a partire da un set di dati di esempio.

Con la programmazione classica è facile risolvere problemi come dover calcolare l'area di un poligono, definendo la logica da adottare. Quindi dando in pasto i dati ad un algoritmo possiamo ottenere un output in maniera deterministica. Un altro esempio è contare il numero di occorrenze di una parola in un testo. 

Ci sono problemi che hanno un grado di incertezza, per esempio quando il problema è troppo difficile da modellare in maniera deterministica, come per esempio classificare un'immagine per definire se un oggetto è presente o determinare se una email è spam. Prendiamo quest'ultimo caso, possiamo analizzare il contenuto delle mail per determinare se sono ham o spam (come per esempio, se ha incoerenze, errori ortografici...).

Generalmente siamo abituati a lavorare con dei dati in input, elaborati da un algoritmo, che forniscono un output. Stavolta vengono presi dati in input con associato il relativo output (etichetta), che vengono dati in input ad un algoritmo di learning, che fornirà in output un programma in grado di risolvere quel problema.

Nel 1997 Tom Mitchel definì il problema del learning, ovvero definire cosa significa apprendere. Definì che un computer apprende da un'esperienza $e$ rispetto un task $t$ e una misura di performance $p$ se le performance $p$ misurate su $e$ migliorano con l'esperienza rispetto al task $t$. Quindi per capire se un algoritmo sta apprendendo dobbiamo misurare le performance.

Il task è il problema da risolvere, e vi è una doppia implicazione con un modello funzione parametrizzata $h$. L'esperienza sono i dati, ognuno di essi è chiamato esempio. Le performance sono una misura che riesca a valutare se il modello è buono rispetto i dati in input, si definisce come $P(\{y^{(i)}\}^m_{i=1}, \{\hat y^{(i)}\}^m_{i=1})$, questo può essere alto o basso. Comunque sia $P\in[0,1]$, di conseguenza definiamo la misura dell'errore $1-P$. 

Per risolvere problemi di machine learning definiamo modelli statistici che dipendono dal task. Un esempio $x=(x_1, x_2, ..., x_d)$, e se ho tanti esempi definisco l'$i$-esimo esempio  $x^{(i)}=(x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_d)$. Per trasformare il vettore riga in vettore colonna basta fare la trasposta del vettore. Le componenti $x_n$ vengono dette in gergo *features*. 

Tornando all'esempio delle mail, vediamo che è necessario attuare un processo di estrazione delle features, quindi trovare una rappresentazione $f(x)=\overline x$ dove $\overline x$ è l'insieme delle features estratte dall'esempio $x$. La rappresentazione è un vero e proprio programma che cerca, per esempio, di trovare errori ortografici e parole chiave dalla mail di spam. La funzione può essere o creata a mano o apprese.

Osservato un problema è bene capire le caratteristiche importanti rispetto al task da risolvere, pertanto non esiste una rappresentazione universale. L'estrazione delle features mette in luce caratteristiche salienti dei dati trascurandone altri. Immaginiamo di avere un grafico con assi numero di parole chiave spam ed errori ortografici, possiamo aspettarci che email di spam abbiano tanti errori e parole chiave spam, diversamente quelle non spam. Definire il modello consiste nel tracciare la linea che separa le mail spam dalle mail ham.

Gli attributi possono essere:

- **Categorical:** nominale (non c'è un ordine relativamente alle features, come il colore) o ordinale (quindi con ordine più o meno espresso, come low, middle e high).
- **Continuous:** numerico

I tipi di task sono:

- **Classificazione:** prendere un input $x\in R^d$ e stabilire a quale categoria appartiene quindi $y \in \{1,...,k\}$ dove $k$ è il numero di classi. Vogliamo definire un classificatore $h_v(x)=y$ con $h:R^4 \rightarrow \{1,...,k\}$. Un altro esempio è determinare quante macchine sono presenti in un immagine, le classi sono i possibili output.
- **Regressione** 

Diversi sono anche i tipi di learning:

- **Supervised Learning:** vengono dati in pasto all'algoritmo sia l'input che l'output associato, così vengono risolti classificazione e regressione. Qui abbiamo sia parametri che iperparametri, nel caso dell'algoritmo KNN dobbiamo prendere k parametri, mentre nelle GMM serviranno anche iperparametri.
- **Unsupervised Learning:** qui viene dato all'algoritmo di learning solo l'input $x^{(i)}\in R$, senza quindi nessuna etichetta. Il loro obiettivo è stimare come i dati si distribuiscono quindi sulla probabilità di riscontrare un certo input. I modelli generativi fanno proprio questo, come il K-means, che divide lo spazio in cluster dove i suoi elementi sono simili rispetto le loro caratteristiche.
- **Reinforcement Learning:** non verrà trattato in questo corso, vi attua una tecnica di premio e penalità rispetto al raggiungimento di un determinato obiettivo.

Definiamo metriche come l'accuracy per la classificazione, quindi la percentuale di risposte corrette date in output dall'algoritmo di learning. Supponiamo di avere 5 mail, 3 spam e 3 ham, tali valutazioni sono dette ground-truth (GT). L'algoritmo valuterà tali mail e contiamo quante volte l'algoritmo ci azzecca. 

Se si parla di regressione consideriamo la misura di performance quella dell'errore, calcolato considerando la differenza tra $y $ di GT e $\hat y$ determinato dall'algoritmo ovvero il *Mean Squared Error (MSE)*.

Come già accennato l'esperienza sono i dati, in genere ne abbiamo un set chiamato dataset che vengono rappresentati con una design matrix
$$
x^{(1)}= x^{(1)}_1\ ... \ x^{(1)}_d \\
x^{(2)}= x^{(2)}_1\ ... \ x^{(2)}_d \\
... \\
x^{(m)}= x^{(m)}_1\ ... \ x^{(m)}_d \\
$$
Le colonne sono le features, ogni riga è un dato. Tale matrice è la design matrix che definisce il dataset, ovvero $X \in R^{mxd}$. Un altra matrice è $Y\in A^{mxk}$ dove $A=\{1,...,k\}$ In genere $k=1$, pertanto $Y$ rappresenta le classi di appartenenza di ogni dato.

Passando alla procedura di learning, spesso detta di training, si ha in input 

- un dataset di training 
- il modello con i relativi parametri
- una misura delle performance

e in output un set di parametri che migliorino le performance sul dataset fornito. Per sapere se l'algoritmo funziona, alla ricezione di nuovi dati l'algoritmo deve saper generalizzare su un nuovo set di dati, detto dataset di test, da dare in pasto all'algoritmo per verificare se l'algoritmo funziona. Qualora avessimo anche gli iperparametri, usiamo anche il dataset di validation. Con i dati di training otteniamo i parametri del modello, con il dataset di validation otteniamo gli iperparametri (indipendenti dal modello) e infine il dataset di test per ottenere l'accuracy.

Un algoritmo di classificazione per classificare $x_1$ potrebbe essere:

```pseudocode
if x1 > theta then "spam"
else "not spam"
```

Sulla base di theta avrò diversa accuracy. Quindi l'algoritmo di learning deve determinarlo. Un metodo naive potrebbe essere provare tutti i theta e verificare quale accuracy è migliore, purtroppo è un'euristica non intelligente perchè l'algoritmo di learning non impara più dai dati, ma dai possibili parametri. Questo processo va bene su una o due dimensioni, ma ovviamente non scala e diventa troppo complesso.

Cerchiamo adesso di normalizzare i dati, per esempio $[0,1]$. Tale normalizzazione la facciamo sui dati di training, e l'algoritmo verrà allenato sui nuovi dati normalizzati.


$$
..
$$
Ovviamente, il dataset di test dovrà pure essere trasformato, per poterlo confrontare col dominio di appartenenza dell'algoritmo.

Abbiamo anche lo Z-score, 



### 1.1 Componenti del ML

Possiamo parlare di algoritmi discriminativi e generativi. In questo corso vedremo quelli discriminativi, supponiamo di avere un grafico con diversi punti mediamente divisi in cluster. Quello che vogliamo fare è determinare un modello che funga da confine di decisione, dividendo i dati in classi. I dati saranno $x \in R^d$, e le etichette $y=\{0,1\}$. Da qui vogliamo sapere la probabilità di assegnare un etichetta a un dato, cioè $P(y|x)$ quindi una probabilità a posteriori.

Nel caso di modelli generativi si ragiona in probabilità congiunta, quindi $P(x,y)$ cioè la probabilità di osservare il punto $x$ appartenente alla classe $y$, così da utilizzare la regola di Bayes per trasformare la probabilità congiunta in una probabilità condizionata quindi a posteriori.

Gli ingredienti del machine learning sono:

1. Modello
2. Loss Function (o misura degli errori) che serve a capire se il modello ha appreso qualcosa durante la fase di training 
3. Algoritmo di learning
4. Evaluation, per valutare il modello

Dobbiamo capire come possiamo noi umani imparare, e il componente principale del cervello è il neurone, la più semplice macchina compotazionale. Da una parte abbiamo i dentriti che prendono input che viene passato al soma, che possiamo vederla come la process unit del neurone, che lo invia all'assone, il quale ritorna un output della computazione, che la inoltra al terminale dell'assone, dove abbiamo diversi percorsi che si ripartono. Da qui il neurone può comunicare con altri neuroni attraverso le sinapsi che stabiliscono una conduzione elettrica. Un neurone che riceve input può essere attivo o non attivo, se tale input è high level allora è attivo, altrimenti se low level non lo attiva e dipende dal neurone. Sostanzialmente prendono un input binario e restituiscono un output binario, low e high, 0 e 1. Combinando vari neuroni, otteniamo la rete neurale (neural network). Sostanzialmente a diversi input corrisponde l'attivazione di determinati neuroni, che a loro volta comuncheranno con gli altri. Si pensa che i neuroni stratificano in livelli in maniera gerarchica, i livello più alti sono atti al riconoscimento di elementi più astratti.

### 1.2  Modello di McColloch-Pitt

L'idea è quella di emulare il comportamento di un neurone, quindi

1. Un'unità computazionale che elabora $n$ input binari
2. Il modello restituisce un output binario
3. Ogni neurone è associato ad un parametro $ \theta$ ovvero threshold (soglia)
4. L'output può essere connesso ad altri neuroni
5. L'input può provenire da altri neuroni o essere un vettore binario o una computazione
6. Due tipi di input: eccitatorio o inibitorio

*immagine neurone*

Se almeno un input è $m\ge 1$ e $\exists y_i|y_i=1$ allora output 0. quindi se almeno uno degli inibitori è 1 il neurone è non attivo
Altrimenti se $\sum_{i=1}^dx_i\ge \theta$ allora output 1
Altrimenti output 0

Se abbiamo un neurone con due input eccitatori e soglia 2 allora il modello è la sommatoria di due input quindi $x_1+x_2\ge \theta$ e dato che la soglia è 2 allora si comporta come la funzione AND. La threshold ce la da la funzione oracolo, quindi dobbiamo conoscere l'output voluto. Sembra difficile ma in effetti nei problemi di supervised learning è possibile.

Possiamo fare anche delle cose più interessanti, come per esempio se abbiamo un neurone con un input inibitorio e un input eccitatorio con threshold 1 allora stiamo computando $x_1+\overline x_2 \ge 1$. Tale modello, come notiamo, è parametrizzato, dove i parametri sono gli input e le threshold. Possiamo creare modelli più complessi usando questo tipo di neurone, magari combinando diversi neuroni creando una network a più livelli.

Ovviamente questo modello ha dei limiti, infatti:

1.  è pensato per ricevere input binario e non numeri interi
2. è pensato per funzioni logiche.
3. I parametri sono fissati a mano
4. Non esistono pesi, due input hanno stessa importanza
5. Tutte le funzioni sono linearmente separabili, cioè le classi sono descrivibili sono solo modelli lineari



### 1.3  Modello del percettrone - Rosenblatt

Consiste nella naturale evoluzione del modello del neurone. Il percettrone può essere visto come un'unità di elaborazione dati, che riceve diversi input, non abbiamo input inibitori ma abbiamo un peso associato ad ogni input detto peso. Tale modello è in grado di computare la combinazione lineare degli input, non solo la somma. L'output va in una nuova unità detta step function, dove abbiamo un altro parametro che definirà la threshold dando un output binario

*immagine*

Tale modello computa:
$$
f(x_1, ..., x_d)=1 \ \ \ \ \ se \ \ \ \sum_{j=1}^dv_jx_v>\theta_0 \\\
f(x_1, ..., x_d)=0 \ \ \ altrimenti
$$
Sostanzialmente gli input appartengono all'insieme dei reali.

Possiamo modificare tale modello spostando la step function all'input si ha che la sommatoria deve essere maggiore di 0 per essere pari a 1.

Se avessimo 3 dimensioni la il confine di separazione non sarebbe una linea ma un piano. 

Come facciamo a settare i parametri? Abbiamo detto che stiamo computando $\sum x_j\theta_j>0$, con il possibile output che è una classificazione binaria. Se $y=0$ e $\hat y = 1$ devo decrementare i parametri per permettere di allinearci alla stima giusta. Può succedere il contrario, quindi voglio in output 1 ma la stima è zero. Significa che la sommatoria non supera lo zero e dobbiamo incrementare i pesi, quindi si vogliamo incrementarlo o decrementarlo a seconda dei casi di un certo $\epsilon$.
$$
\theta_j \leftarrow \theta_{j}+(y-\hat y)x_j
$$
Questa formula funziona in entrambi i casi sopracitati sfruttando gli errori che abbiamo.

L'operazione che faremo è $\theta \leftarrow \theta_j  \pm \epsilon$.

I passi sono:

1. Inizializza i pesi random in [0,1]
2. Considera $x^{(1)}$ e computa $\hat y^{(i)}$ e questo $\forall i$
3. Per le predizioni errate aggiusta i pesi:
   1. Se $y=0$ e $\hat y = 1$ decrementa
   2. Se $y=1$ e $\hat y = 0$ incrementa
4. Ritorna al passo 2 fino a convergenza



### 1.4  XOR Problem

Questa categoria di problemi non è rappresentabile con modelli lineari a causa del loro grafico

*immagine grafico*

Come possiamo risolverlo? Pensiamo di avere una rete neurale a percettroni così fatta:

*rete a percettroni*
$$
f_1(x_1,x_2)= [x_1-x_2>0.5] \\
f_2(x_1,x_2)= [x_2-x_1>0.5] \\ 
f_3(f_1(x_1,x_2), f_2(x_1,x_2)) = 1*f_1(x_1,x_2) + 1*f_2(x_1,x_2) > 0.5
$$
Tale modello descrive proprio la soluzione al problema dello XOR. Infatti le tabelle di verità corrispondono. L'input praticamente viene trasformato e dato in input al terzo neurone descritto da $f_3$. Dal grafico notiamo che abbiamo collassato i punti in cui deve essere falso nel punto $(0,0)$, e possiamo dividere linearmente!

Solo che, come facciamo learning qui? In effetti non possiamo fare learning in questo caso come abbiamo visto nel capitolo scorso, perchè non possiamo sistemare i pesi per risolvere questo tipo di network che può risolvere problemi più complessi, non sappiamo a priori quali dovrebbero essere i pesi del livello intermedio (spoiler: risolto con la backpropagation).

## 2  Regressione

La regressione è una tecnica di leaerning supervisionato che data una funzione parametrizzata $f:R^n\rightarrow R^{n'}$ cerca di approssimare la relazione tra input e output dove $n$ è il numero di feature ed $n'$ è il numero di output.

Per costruire un modello di regressione ci servono:

1. Un modello (la funzione parametrizzata, per esempio se regressione lineare una retta)
2. Loss Function (Misura degli errori delle predizioni e i dati reali)
3. Algoritmo di learning (Un metodo per aggiornare i parametri per es. discesa del gradiente)
4. Valutazione del modello

Ci serviranno dei dati per:

-  Training
- Validation
- Test (per valutare il modello)

Ricordiamo che siamo in learning supervisionato, quindi nel training abbiamo dei dati associati alla loro etichetta.

Immaginiamo di avere:
$$
Y_{test} = \{y^{(i)}\}_{i=1}^m \\
\hat Y = \{f_\theta (x^{(i)}) \}_{i=1}^m
$$
Ricordiamo che $m$ è il numero di dati che abbiamo nel dataset. $Y_{test}$ è un insieme di campioni, non di dati! Si intende quindi che l'$i$-esimo campione avrà struttura $Y^{(i)}=y_1^{(i)},...,y_{n'}^{(i)}$. Di conseguenza gli output  $\hat Y^{(i)}=y_1^{(i)},...,y_{n'}^{(i)}$.

Possiamo valutare la regressione lineare utilizzando il Mean Square Error, cioè una distanza euclidea. Que ovvero:
$$
||\hat y^{(i)} - y^{(i)}||_2 = \sqrt{\sum_{j=1}^{n'}(\hat y^{(i)}_j - y^{(i)}_j)^2}
$$
E di conseguenza l'errore:
$$
Error(Y_{test}, \hat Y_{test}) = \frac1m \sum_{i=1}^m{||\hat y^{(i)} - y^{(i)}||_2}
$$
Volendo potremmo anche togliere la radice quadrata, peggiorando diciamo l'importanza gli errori che possono nascere.
$$
MSE = ||\hat y^{(i)} - y^{(i)}||_2 = \sum_{j=1}^{n'}(\hat y^{(i)}_j - y^{(i)}_j)^2
$$
Abbiamo definito la regressione come una funzione parametrizzata $f:R^n \rightarrow R^{n'}$, ma ci sono tre casi specifici:

1. $n=n'=1$ :arrow_right: Simple Regression quindi $f_\theta(x)=y$, con $\theta_0+x_1+\theta_1$
2. $n>1$, $n'=1$ :arrow_right: Multiple Regression quindi $f_\theta(x_n) = y$ con $\theta_0 + x_1\theta_1+...+x_n\theta_n = \vec\theta^T\vec x$.
3. $n>1$, $n'>1$ :arrow_right: Multivariate Regression avremo sostanzialmente $n$ regressioni lineari ognuna di esse darà un output

Con la Linear Regression possiamo descrivere i dati con una retta.

Definiamo la Loss Function, prende in input il vettore dei parametri e deve stimare l'errore sui dati di training
$$
J(v) = \frac12 \sum_{i=1}^m (f_v(x^{(i)})-y^{(i)})
$$
Tale funzione è differenziabile e questo è importante per renderla riutilizzabile per altri modelli. Di tale funzione vogliamo trovare il minimo, minimizzare la quantità di errore quindi trovare i parametri che ci consentono tale situazione. Una volta trovata, fissiamo i parametri e li utilizziamo nella fase di test.

Ad ogni iterazione cambieremo i parametri, facendo scendere la quantità degli errori. Per farlo, mando un input random e calcolo la derivata, se è negativa significa che mi conviene aggiungere qualcosa, viceversa se positiva significa che mi conviene togliere qualcosa. Tale algoritmo si chiama discesa del gradiente. Via via raggiungeremo il punto di zero, che sarà il punto di minimo della funzione. Possiamo impostare anche un parametro detto learning rate che ci permette di velocizzare il learning.
$$
v^t_j =v_j^{t-1} - \gamma \frac {\partial \delta  } {\partial v_j }
$$
Abbiamo parlato di feature scaling, se non le scaliamo la discesa del gradiente potrebbe far fatica per raggiungere il minimo, quindi meglio normalizzare.

Andando avanti
$$
J(v) = \frac12 \sum_{i=1}^m (f_v(x^{(i)})-y^{(i)}) = \frac 12 \sum_{i=1}^m 
$$








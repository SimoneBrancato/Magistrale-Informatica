# Quantum Information

[TOC]



## 1  Introduzione

### 1.1 Che cos'è la teoria dell'informazione?

Ci si chiede:

- Fino a che punto posso comprimere dati senza perdere informazione? Non vanno bene le compressioni di tipo lossy
- Qual è il miglior tasso di trasmissione per la comunicazione? Ogni volta che trasmettiamo dati lo facciamo su mezzi inaffidabili, quindi quanta informazione posso trasmettere in maniera affidabile (tasso di errore prossimo a zero) su un determinato mezzo? aggiungere ridondanza non permette di migliorare molto la situazione, perchè dovremmo aggiungere ridondanza infinita per avere tasso di errore nullo

Ha applicazioni in:

- Fisica (termodinamica)
- Informatica (Complessità, Pattern Recognition e altri)
- Matematica (probabilità)
- Economia (andamento delle borse)

Cercheremo di capire come misurare l'informazione, come comprimere dati e comunicare su canali inaffidabili. Un esempio sono le linee telefoniche analogiche, comunicazioni con onde radio. Un'altra applicazione sono le trasmissioni del DNA nella riproduzione delle cellule, o scrittura su Hard Disk. 

Possiamo utilizzare un sender e un receiver, prima del canale di comunicazione abbiamo l'encoder e appena dopo il decoder così da comprimere i dati in trasmissione. Ciò che vogliamo è che i dati in input e output siano gli stessi. La teoria dell'informazione studia proprio i limiti e le potenzionalità di queste soluzioni. Standardizzare certe soluzioni permette di minimizzare i costi.

Tutto si basa sulla teoria di Shannon, che definisce il poter inviare informazione con probabilità di errore $\epsilon$, tendente a zero, e per farlo dobbiamo definire il tasso di comunicazione, minore della capacità del canale. Nella trasmissione del segnale dobbiamo stare attenti alle sue caratteristiche, che definiscono la sua complessità intrinseca ovvero quanto può essere compresso, viene detta entropia, ed è il limite invalicabile per la compressione di quel dato. Se l'entropia è minore della capacità del canale allora possiamo effettuare comunicazioni senza errori, è matematicamente possibile. In realtà la teoria suggerisce soluzioni non fattibili nella pratica perchè troppo inefficienti, non parleremo propriamente delle soluzioni ma delle idee che stanno alla loro base.

### 1.2  Che cos'è la teoria dell'informazione quantistica?

Ci concentreremo sui dettagli tecnici di come funzionano i supporti di memorizzazione, sappiamo che se i transistor sono molto piccoli sono molto veloci, ad un certo punto non possiamo ignorare gli effetti quantistici. Supporre che il sistema segue le leggi dell'informazione quantistica cosa implica? L'equivalente quantistico del bit è il qubit, piuttosto che essere 0/1 si può trovare in una loro sovrapposizione, che può essere misurata in probabilità. Tali effetti possono essere simulati con sistemi classici, ma quello che distingue la quantum information sono i termini di coerenza usando matrici densità. Inoltre, certi sistemi quantistici possono essere indipendenti, correlati classicamente o entangled, quindi utilizzando la correlazione non classica dell'entanglement quantistico.

Vedremo come misurare l'entropia di uno stato quantistico (Entropia di Von Neumann) e la compressione. Parleremo di trasmissione di bit (capacità classica), qubit non entangled (capacità quantistica) e qubit entangled (capacità quantistica assistita).

Approfondire: Sfera di Bloch

### 1.3  Obiettivi del corso, prerequisiti, informazioni generali

Fornire una panoramica semplice ma rigorosa della teoria dell'informazione classica e quantistica.

Tutto ciò che viene fatto a lezione è oggetto d'esame.

Libro principale prima parte: MacKay - Information Theory, Inference and Learning Algorithms (Scaricabile online)

Libri seconda parte: vedi syllabus, tutti scaricabili

- Watrous
- Mermin più per informatici
- Nielsen più per fisici

 Esame: 

- Due prove in itinere, una per parte, esercizi teorici e pratici, chi supera 18 di media allora verrà proposto il voto con aggiunta di un bonus
- Scritto con orale

## 2  Teoria dell'informazione classica

### 2.1  ..

Consideriamo un canale che ha una sua probabilità di errore, strettamente compresa tra 0 e 1.
$$
0 < f < 1
$$
Vogliamo trasmettere un messaggio, quindi ogni bit viene trasmesso uno per uno e ogni bit ha probabilità di errore pari a $f$. Ovviamente non ci interessano i casi estremi perchè sarebbe un canale perfetto. Abbiamo sender e receiver. Questo si chiama canale binario simmetrico, dove se il sender manda un bit allora c'è una probabilità pari a $1-f$ che il receiver riceva lo stesso valore del bit, e una probabilità $f$ che riceva il bit alterato.

Supponiamo che $f=0.1$, significa che un bit su 10 è alterato e non sappiamo quali sono alterati. In un contesto di trasmissioni multiple, tale probabilità è inaccettabile. Mandare semplicemente i bit è la decodifica identità. Una possibile soluzione è utilizzare i codici di ripetizione: attualmente stiamo codificando ogni bit con un bit, piuttosto adesso ogni bit lo codifico con 3 bit. Ovviamente scegliamo un numero dispari per evitare parità.

Supponiamo di voler mandare la stringa 0110, usando il codice di ripetizione con ripetizione 3 allora definisco

​	$R_3$ codice di ripetizione con $n=3$, si ha: $000\ 111\ 111\ 000$

Utilizziamo la decodifica della maggioranza, se ho $n=3$ ragiono per chunk di 3 bit, ogni 3 bit considero la maggioranza, e per la decodifica assegno il bit più presente. Qualora si verificassero errori, è meno probabile che nel chunk si verifichino così tanti errori da alterarne la maggioranza, ma è davvero scomodo: moltiplico per un fattore pari a $n$ il numero di bit da inoltrare rispetto la decodifica identità, e comunque qualche errore ce lo ritroviamo perchè non li correggiamo. 

In questo contesto, la probabilità di errore è pari a quando si verificano nella singola tripletta 2 o 3 errori.
$$
P(E)= \sum_{i=2}^3\binom3i f^i(1-f)^{3-i}=    \binom{3}{2}f^2(1-f)\ +\ f^3 = \\\
= 3f^2-3f^3+f^3 = 3f^2-2f^3 \approx O(n^2)
$$
La sequenza è:

​	$b$ $\rightarrow$ Encorder $\rightarrow$ $S_b$ $\rightarrow$ Canale rumoroso $\rightarrow$ $S'_b$ $\rightarrow$ Decoder $\rightarrow$ $b'$

E quindi $P(E)=P(b\ne b')$.

In questo modo, prima avevamo una probabilità del 10% di errore, adesso 0.01%. Potremmo pensare: facile, aumentiamo $n$. 
Consideriamo anche che l'analisi che stiamo facendo fa riferimento a un canale simmetrico, qualora per esempio il sender dimentichi di mandare un bit potremmo avere un disastro.

Generalizzando possiamo definire $R_N$, si ha $P(E_N)=\sum_{i=\frac{n+1}{2}}^N \binom Ni f^i(1-f)^{N-i} \approx O(f^{\frac{n+1}{2}})$.
L'errore non decresce linearmente, notiamo che aggiungendo bit inizia ad essere sempre meno vantaggioso aggiungere ridondanza. Per un errore circa zero dovremmo avere ridondanza infinita e non va bene.

Ancora, se definissimo $R_9$ si ha $P(E_9)\approx O(f^5)$.

Cambiamo approccio, definiamo $R^2_3$, per esempio pensiamo di mandare 1101, se applico $R_3$ due volte ricorsivamente ottengo un codice lungo come $R_9$, ma stavolta il decoder è più efficiente perchè prima legge la stringa da 9 bit divisa per 3 chunk, e poi sui 3 chunk nuovamente calcola la maggioranza e ha decodificato il bit.

Dato $E_{R^2_3}$, si ha $P(E_{R^2_3})=\sum_{i=2}^3\binom3i p^i(1-p)^{3-i} \approx f^4$ perchè $p\approx f^2$. Quindi comunque è peggio di $R_9$.

Definiamo il tasso di trasmissione dei bit (rate), quindi il rapporto tra numero di bit che vogliamo inviare e il numero di bit effettivamente inviati. Con $R_9$ avremmo un rate pari a $\frac19$. 

Possiamo quindi utilizzare il **codice di Hamming (7,4)**:

- Sequenza da mandare $S_1 S_2 S_3 S_4 $ con $t_1t_2t_3$. Qui il rate in effetti è $\frac47$.
- Immagino di avere 3 cerchi a diagramma di Venn. Nelle 4 intersezioni inseriamo i 4 bit da mandare, mentre le tre t le inserisco nei campi senza intersezione
- Scelgo $t_1$ tale che $S_1+S_2+S_3+t_1= 0 \mod 2$
- In formula ci stanno le $S$ che fanno parte dello stesso cerchio di $t_1$
- Procedo analogamente per le altre $t$, sostanzialmente sono bit di parità e quell'operazione è XOR.

Quindi avremmo per $1101$ bit di parità $000$. Trasmettiamo nel canale inaffidabile, e riceviamo $1001\ 000$. Facendo il disegno notiamo che i bit di parità non funzionano, e dobbiamo fare il cambiamento che sistemi la situazione col minor numero di modifiche. Due cerchi su tre sono a posto, pertanto il cerchio okay non deve essere toccato. In effetti, notiamo che possiamo correggere l'intersezione, ripristinando la parità.

Se i primi due cerchi non sono corretti possiamo definire il codice sindrome classificando i cerchi scorretti, qui è $110$. Con sindrome $000$, allora tutto a posto. 

E se ci fosse più di un errore? Ipotizziamo di ricevere $1011\ 000$ e disegnamo il diagramma di Venn

- Notiamo che il terzo cerchio non è corretto e la sindrome è $001$
- Esiste un modo per azzerare la sindrome, ed è porre $t_3 =1$
- Secondo l'algoritmo di decodifica la soluzione sarebbe $1011\ 001$
- Ma in realtà avevamo mandato $1101\ 000$

La probabilità di errore associata a questo metodo è definito dall'evento $E_H$ dove la sequenza delle $S$ viene ricevuta scorrettamente. Si ha:
$$
P(E_H)=\sum_{i=2}^7\binom7i f^i(1-f)^{7-i} \approx f^2
$$
Riusciamo a gestire un solo errore, con più errori è un disastro. La cosa buona è che abbiamo migliorato il tasso di trasmissione. Sembrerebbe che se volessimo un tasso di errore basso, dovremmo avere anche un tasso di trasmissione bassissimo, e non va bene perchè stiamo decidendo tra affidabilità ed efficienza. Questo problema venne risolto da Shannon, garantendo tassi di errore infinitesimali.





## 3  Teoria dell'informazione quantistica



### 3.1  Prerequisiti di algebra lineare

Consideriamo lo spazio di Hilbert sui complessi $C$, che è uno spazio vettoriale su cui è definito un prodotto scalare, è detto dei vettori colonna sui numeri complessi
$$
v = (v_0,v_1,...,v_{d-1}), \ v_i \in C
$$
 dotato del prodotto scalare standard $<v|w>=\sum_{i=0}^{d-1}\overline v_i w_i$.

Se H è uno spazio di Hilbert arbitrario con $dim(H)=d$ finita possiamo scegliere una base ortonormale $\{k_i>\}^d_{i=1}$ di H cioè tale che è 0 o 1 (rivedi slide).

Indichiamo i vettori di H con il simbolo |Phi> (ket) e usiamo <Phi| (bia) per il suo duale, il quale è un funzionale quindi bia:H -> C

Quindi ket -> braket



Utilizziamo la base ortonormale standard di $H = C^d$ quindi sarebbe

|0> = (1 0 0 ... 0)      |1> = (0 1 0 0 ... 0) ....   |d-1> = (0 0 ... 0 1)

Se |phi> = ( phi0 phi1 ... phi_d-1) allora $|\Psi> = \sum_{i=0}^{d-1} \Phi_i|i>$

Il duale di ket è $bia = (\overline \Psi_0 ...\overline \Psi_{d-1})$

Il prodotto scalare soddisfa la disuguaglianza di Schwartz: $\forall |\Phi>, |\Psi> \in H = C^d$ allora $|<\Phi | \Psi>|^2 \le  ...$

La norma indotta dal prodotto scalare è 

Possiamo risceivere la disuguaglianza di Schwartz come 

### 3.2  Mappe lineari e matrici

Una mappa lineare è una mappa che soddisfa le proprietà:

1. $f(\lambda x) = \lambda f(x)$
2. $f(x+y) = f(x) + f(y)$

Dati $H=C^d$ e $K=C^d$ spazi di Hilbert, $Lin(H, K)$ è l'insieme delle mappe lineari da H a K. Se fissiamo una base per $H$ e per $K$ allora ogni mappa lineare di $Lin(H,K)$ può essere scritta come una matrice $d xe$ . Inoltre useremo $Lin(H)=Lin(H,H)$. Inoltre avremo l'operatore identità 1.

La traccia di una $M \in Lin(H)$ si calcola scegliendo una base $\Sigma$ di H e sommando gli elementi della diagonale principale.
Qualunque base scegliamo, la traccia è la stessa quindi la traccia è un invariante per cambiamenti di base e soddisfa
$$
tr[MN] = tr[NM] ......
$$


...



Se abbiamo un'applicazione lineare $M\in Lin(H)$ e $\exists ket \in H$ non nullo e $\lambda \in C$ tale che .... ket pè un autovettore di M con autovalore $\lambda$.

Se $M\in Lin (H_1, H_2)$ dove $H_1$ ha base $\Sigma_1$ e analogmente H2 allora M = ...

Se $M \in Lin(H_1, H_2)$ l'operatore aggiunto di M è dagger $M^+= \overline M^T$ ed è tale che 

- $(MN)^+ = N^+M^+$ 

Operatori Hermitiani

è una matrice che ha la proprietà di essere autoaggiunta, cioè che $M = M^+$. (riprendi) quindi la diagonale principale ha solo reali e $M_{ij} = M_{ji}$.

**Teorema Spettrale** Sia M \ in Lin (H) Hermitiano e dim(H) = d allora \exists \lambda_1 ... e una base tali che M = \sum_{i=1}^d\lambda 

Dice che una matrice Hermitana può essere diagonalizzata usando matrici mitarie e la matrice diagonale ha eleme

 























































